{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk, streaming_bulk\n",
    "from elasticsearch.exceptions import TransportError\n",
    "import json\n",
    "import codecs\n",
    "import pprint\n",
    "import glob\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'SIHSUS'\n",
    "\n",
    "region = 'rj'\n",
    "year = 2009\n",
    "month = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variaveis e parâmetros do Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticSearch settings\n",
    "ES_CLUSTER = ['http://localhost:32787']\n",
    "ES_INDEX = 'datasus'\n",
    "ES_TYPE = 'SIHSUS'\n",
    "\n",
    "elastic_search_client = Elasticsearch(ES_CLUSTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar se Index existe e criar caso nao exista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o indice para ser utilizado, (TODO ->)caso exista, e deseje deletar, \n",
    "# entrar no console do kibana em Management, selecionar o index e clicar \n",
    "# em manage index e deletar index e rodar novamente\n",
    "# Importante para primeiro acesso\n",
    "def create_index_or_recreate(elastic_search_client, index):\n",
    "    create_index_body = {\n",
    "      'settings': {\n",
    "        # just one shard, no replicas for testing\n",
    "        'number_of_shards': 1,\n",
    "        'number_of_replicas': 0\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # create empty index\n",
    "    try:\n",
    "        elastic_search_client.indices.create(\n",
    "            index=index,\n",
    "            body=create_index_body\n",
    "        )\n",
    "    except TransportError as e:\n",
    "        # ignore already existing index\n",
    "        if e.error == 'index_already_exists_exception':\n",
    "            print('index - index alread_exists')\n",
    "\n",
    "            pass\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de dados para indexacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index_or_recreate(elastic_search_client, ES_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar as chaves para letras minusculas pq o elastic search eh case sensitive\n",
    "# Isso evita duplicatas  erradas (Mes == mes)\n",
    "def normalize(data, ES_INDEX, ES_TYPE):\n",
    "    data = data.to_dict('records') \n",
    "    \n",
    "    for row in  data:\n",
    "        yield { \"_index\": ES_INDEX,\n",
    "               \"_type\": ES_TYPE,\n",
    "               \"_source\":{str(k).lower(): v for k, v in row.items()}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_from_datasus_by(region, year, month):\n",
    "    d = [{'col1' : 1, 'col2': 2}, {'col1' : 3, 'col2': 4}]\n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregar o dado no ELK em streaming\n",
    "def load_repo(elastic_search_client, data, ES_INDEX, ES_TYPE):\n",
    "\n",
    "    # quantidade de itens processados com sucesso e falha\n",
    "    success, failed = 0, 0\n",
    "    \n",
    "    # list of errors to be collected is not stats_only\n",
    "    errors = []\n",
    "    \n",
    "    # se true somente printar a informacao do erro e contar as  falhas\n",
    "    # se false, acumular os erros na lista criada acima\n",
    "    stats_only = True\n",
    "    \n",
    "    pprint.pprint(data)\n",
    "    pprint.pprint('for a do streaming ' + ES_TYPE)\n",
    "\n",
    "    # we let the streaming bulk continuously process the commits as they come\n",
    "    # in - since the `parse_commits` function is a generator this will avoid\n",
    "    # loading all the commits into memory\n",
    "    for ok, result in streaming_bulk(\n",
    "            elastic_search_client,\n",
    "            data,\n",
    "            index=ES_INDEX,\n",
    "            doc_type=ES_TYPE\n",
    "        ):\n",
    "\n",
    "        action, result = result.popitem()\n",
    "        doc_id = '/%s/doc/%s' % (ES_INDEX, result['_id'])\n",
    "        # process the information from ES whether the document has been\n",
    "        # successfully indexed\n",
    "        if not ok:\n",
    "            if not stats_only:\n",
    "                errors.append(result)\n",
    "            print('Failed to %s document %s: %r' % (action, doc_id, result))\n",
    "            failed += 1\n",
    "        else:\n",
    "            print(doc_id)\n",
    "            success += 1\n",
    "    return success, failed, errors\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(elastic_search_client, region, year, month):\n",
    "    data = get_dataframe_from_datasus_by(region, year, month)\n",
    "    \n",
    "    # garantindo que poderemos rastrear os dados \n",
    "    data['_region'] = region\n",
    "    data['_year'] = year\n",
    "    data['_month'] = month\n",
    "    \n",
    "    data = normalize(data, ES_INDEX, ES_TYPE)\n",
    "\n",
    "    success, failed, errors = load_repo(elastic_search_client, data, ES_INDEX, ES_TYPE)\n",
    "    pprint.pprint('Sucess quantity: ' + str(success))\n",
    "    pprint.pprint('Failed quantity : ' + str(failed))\n",
    "    pprint.pprint('Errors : ' + str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    csv_file = '/Users/rafaelstomaz/.tmpEtlDATASUS//DNBA2015.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'2015'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = csv_file.split('/')[-1]\n",
    "region = file_name[2:4]\n",
    "year = file_name[4:8]\n",
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run2(elastic_search_client, csv_file):\n",
    "    file_name = csv_file.split('/')[-1]\n",
    "    region = file_name[2:4]\n",
    "    year = file_name[4:8]\n",
    "    \n",
    "    data = pd.read_csv(csv_file)\n",
    "    data = data.where(pd.notnull(data), None)\n",
    "    \n",
    "    # garantindo que poderemos rastrear os dados \n",
    "    data['_region'] = region\n",
    "    data['_year'] = year\n",
    "    #data['_month'] = month\n",
    "    \n",
    "    data = normalize(data, ES_INDEX, ES_TYPE)\n",
    "\n",
    "    success, failed, errors = load_repo(elastic_search_client, data, ES_INDEX, ES_TYPE)\n",
    "    pprint.pprint('Sucess quantity: ' + str(success))\n",
    "    pprint.pprint('Failed quantity : ' + str(failed))\n",
    "    pprint.pprint('Errors : ' + str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /Users/rafaelstomaz/.tmpEtlDATASUS//DNBA2015.csv does not exist: '/Users/rafaelstomaz/.tmpEtlDATASUS//DNBA2015.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-e22aaf611706>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrun2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melastic_search_client\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcsv_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-21-8b5fddaf9fe3>\u001B[0m in \u001B[0;36mrun2\u001B[0;34m(elastic_search_client, csv_file)\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0myear\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfile_name\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotnull\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mparser_f\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[1;32m    674\u001B[0m         )\n\u001B[1;32m    675\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 676\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    677\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    678\u001B[0m     \u001B[0mparser_f\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    446\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    447\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 448\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp_or_buf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    449\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    450\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    878\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    879\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 880\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    881\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    882\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1112\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mengine\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"c\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1113\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"c\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1114\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCParserWrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1115\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1116\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mengine\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"python\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m   1889\u001B[0m         \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"usecols\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0musecols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1890\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1891\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparsers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTextReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1892\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munnamed_cols\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munnamed_cols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1893\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] File /Users/rafaelstomaz/.tmpEtlDATASUS//DNBA2015.csv does not exist: '/Users/rafaelstomaz/.tmpEtlDATASUS//DNBA2015.csv'"
     ]
    }
   ],
   "source": [
    "run2(elastic_search_client, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}